import React, { useEffect, useRef, useState } from 'react';
import * as faceapi from 'face-api.js';
import Title from '../components/Title';

function LiveEmotion() {
    const videoRef = useRef(null);
    const canvasRef = useRef(null);
    const [message, setMessage] = useState(''); // √âtat pour afficher un message

    // Traductions des √©motions
    const emotionTranslations = {
        happy: 'heureux üòä',
        sad: 'triste üò¢',
        angry: 'en col√®re üò°',
        surprised: 'surpris üòÆ',
        neutral: 'neutre üòê',
        disgusted: 'd√©go√ªt√© ü§¢',
        fearful: 'peur üò±',
    };

    useEffect(() => {
        const loadModels = async () => {
            try {
                // Charger les mod√®les n√©cessaires
                await faceapi.nets.tinyFaceDetector.loadFromUri('/models');
                await faceapi.nets.faceLandmark68Net.loadFromUri('/models');
                await faceapi.nets.faceExpressionNet.loadFromUri('/models');
                console.log('Mod√®les charg√©s avec succ√®s');
            } catch (err) {
                console.error('Erreur lors du chargement des mod√®les :', err);
            }
        };

        const startVideo = async () => {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                if (videoRef.current) {
                    videoRef.current.srcObject = stream;
                }
            } catch (err) {
                console.error('Erreur lors de l‚Äôacc√®s √† la cam√©ra :', err);
            }
        };

        const handleVideoPlay = () => {
            const interval = setInterval(async () => {
                if (videoRef.current) {
                    const detections = await faceapi
                        .detectAllFaces(videoRef.current, new faceapi.TinyFaceDetectorOptions())
                        .withFaceLandmarks()
                        .withFaceExpressions();

                    const canvas = canvasRef.current;
                    const displaySize = {
                        width: videoRef.current.videoWidth,
                        height: videoRef.current.videoHeight,
                    };

                    faceapi.matchDimensions(canvas, displaySize);

                    const resizedDetections = faceapi.resizeResults(detections, displaySize);

                    const ctx = canvas.getContext('2d');
                    ctx.clearRect(0, 0, canvas.width, canvas.height);

                    if (resizedDetections.length === 0) {
                        setMessage('Aucun visage d√©tect√©.'); // Afficher un message si aucun visage n'est d√©tect√©
                    } else {
                        setMessage(''); // R√©initialiser le message si un visage est d√©tect√©
                        faceapi.draw.drawDetections(canvas, resizedDetections);
                        faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);

                        // Traduire et afficher les √©motions d√©tect√©es
                        resizedDetections.forEach((detection) => {
                            const { x, y } = detection.detection.box;
                            const expressions = detection.expressions;
                            const sortedEmotions = Object.entries(expressions).sort(
                                (a, b) => b[1] - a[1]
                            ); // Trier par score d√©croissant
                            const topEmotion = sortedEmotions[0]; // √âmotion la plus probable
                            if (topEmotion) {
                                const [emotion, score] = topEmotion;
                                const translatedEmotion =
                                    emotionTranslations[emotion] || emotion; // Traduire en fran√ßais
                                ctx.fillStyle = 'red';
                                ctx.font = '20px Arial';
                                ctx.fillText(
                                    `${translatedEmotion} (${Math.round(score * 100)}%)`,
                                    x,
                                    y - 30
                                );
                            }
                        });
                    }
                }
            }, 300);

            return () => clearInterval(interval);
        };

        // Charger les mod√®les et d√©marrer la cam√©ra
        loadModels();
        startVideo();

        if (videoRef.current) {
            videoRef.current.addEventListener('play', handleVideoPlay);
        }

        return () => {
            if (videoRef.current) {
                videoRef.current.removeEventListener('play', handleVideoPlay);
            }
        };
    }, []);

    return (
        <div className="main flex flex-col bg-black min-h-screen justify-center items-center p-5 mt-10 gap-5">
            <Title title="Live Emotion" />
            <p className="text-white">D√©tection des √©motions en direct gr√¢ce √† votre cam√©ra.</p>
            <div className="live-emotion">
                <video
                    ref={videoRef}
                    autoPlay
                    muted
                />
                <canvas
                    ref={canvasRef}
                />
            </div>
            {/* Message en dessous */}
            {message && (
                <p className="text-red-500 text-center mt-5">{message}</p>
            )}
        </div>
    );
}

export default LiveEmotion;
